{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5057493"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "res=requests.get(\"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/bbc-text.csv\")\n",
    "open(\"./tmp/bbctext.csv\",\"wb\").write(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2225\n",
      "2225\n",
      "radcliffe enjoys winning comeback paula radcliffe made triumphant return competitive running victory new york marathon. briton running first time since dropping olympic marathon 10 000m held off kenyan susan chepkemei thrilling finish. pair locked together last miles radcliffe finally sprinted clear win two hours 23 minutes 10 seconds. s good way end year said. m ready good rest now. radcliffe decided recently run race many doubted whether sufficiently recovered olympic ordeal just 11 weeks ago. world record-holder prominent head field whole race rivals slowly dropped off pace. just chepkemei radcliffe left contention race came final miles. kenyan put several bursts speed throw off radcliffe briton managed hang in. runners looked suffering reached final mile central park. radcliffe managed dredge final sprint see off chepkemei closest finish race s history process make huge step erasing disappointment suffered athens.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "sentences=[]\n",
    "labels=[]\n",
    "\n",
    "with open(\"./tmp/bbctext.csv\",\"r\") as f:\n",
    "    datastore=csv.reader(f,delimiter=\",\")\n",
    "    next(datastore)\n",
    "    for text in datastore:\n",
    "        sentence=text[1]\n",
    "        for word in stopwords:\n",
    "            token= \" \"+word+\" \"\n",
    "            sentence=sentence.replace(token,\" \")\n",
    "            sentence=sentence.replace(\"  \",\" \")\n",
    "        sentences.append(sentence)\n",
    "        labels.append(text[0])\n",
    "print(len(sentences))\n",
    "print(len(labels))\n",
    "print(sentences[546])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29714\n"
     ]
    }
   ],
   "source": [
    "tokenizer=Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index=tokenizer.word_index\n",
    "print(len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2225, 2442)\n",
      "[  73 4666 4051 ...    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "sequences=tokenizer.texts_to_sequences(sentences)\n",
    "padded=pad_sequences(sequences,padding=\"post\")\n",
    "print(padded.shape)\n",
    "print(padded[67])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'business': 2, 'tech': 4, 'entertainment': 5, 'sport': 1, 'politics': 3}\n"
     ]
    }
   ],
   "source": [
    "label_tokenizer = Tokenizer()\n",
    "label_tokenizer.fit_on_texts(labels)\n",
    "label_word_index = label_tokenizer.word_index\n",
    "label_seq = label_tokenizer.texts_to_sequences(labels)\n",
    "#print(label_seq)\n",
    "print(label_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
